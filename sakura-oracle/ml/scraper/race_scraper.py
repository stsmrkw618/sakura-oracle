"""
SAKURA ORACLE â€” netkeibaã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰

ä½¿ã„æ–¹:
    PYTHONIOENCODING=utf-8 py ml/scraper/race_scraper.py

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. é–‹å‚¬æ—¥ãƒšãƒ¼ã‚¸ â†’ ãƒ¬ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ä¸€è¦§ â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ race_id ç‰¹å®š
    2. race_id â†’ race.netkeiba.comï¼ˆä¸ŠãŒã‚Š3Fãƒ»é€šéé †ã‚ã‚Šï¼‰ã‚’å„ªå…ˆå–å¾—
    3. å–å¾—å¤±æ•—æ™‚ã¯ db.netkeiba.com ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    4. å…¨çµæœã‚’ CSV ã«çµ±åˆå‡ºåŠ›
"""

import time
import random
import pickle
import re
import sys
from datetime import datetime, timedelta
from io import StringIO
from pathlib import Path

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

import requests
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))
from ml.scraper.config import (
    HEADERS, CACHE_DIR, RAW_DIR,
    TARGET_RACES,
    REQUEST_TIMEOUT, MAX_RETRIES, MIN_WAIT, MAX_WAIT, BACKOFF_BASE,
)


def polite_sleep() -> None:
    """netkeibaç”¨ã®ç¤¼å„€æ­£ã—ã„å¾…æ©Ÿï¼ˆ3-7ç§’ãƒ©ãƒ³ãƒ€ãƒ ï¼‰"""
    time.sleep(random.uniform(MIN_WAIT, MAX_WAIT))


def safe_request(url: str, max_retries: int = MAX_RETRIES) -> bytes | None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãå®‰å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    cache_key = re.sub(r'[^a-zA-Z0-9]', '_', url)[-100:]
    cache_file = CACHE_DIR / f"{cache_key}.pkl"

    if cache_file.exists():
        with open(cache_file, "rb") as f:
            return pickle.load(f)

    for attempt in range(max_retries):
        try:
            polite_sleep()
            r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)

            if r.status_code == 400:
                backoff = BACKOFF_BASE * (attempt + 1)
                print(f"  âš ï¸ 400ã‚¨ãƒ©ãƒ¼ã€‚{backoff}ç§’å¾…æ©Ÿ...")
                time.sleep(backoff)
                continue
            if r.status_code == 404:
                print(f"  âš ï¸ 404: {url}")
                return None

            r.raise_for_status()
            r.encoding = r.apparent_encoding or "euc-jp"

            with open(cache_file, "wb") as f:
                pickle.dump(r.content, f)
            return r.content

        except requests.RequestException as e:
            print(f"  ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_retries}: {e}")
            time.sleep(30)

    print(f"  âŒ å–å¾—å¤±æ•—: {url}")
    return None


# æ‰‹å‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¥ä»˜æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã„ãƒ¬ãƒ¼ã‚¹ã®race_id
MANUAL_RACE_IDS: dict[str, str] = {
    # å¿…è¦ã«å¿œã˜ã¦ "label": "race_id" ã§è¿½åŠ 
    # ä¾‹: "ãƒ•ã‚§ã‚¢ãƒªãƒ¼S2025": "202506010511",
}


def find_race_id_from_date(date: str, keyword: str, label: str = "") -> str | None:
    """é–‹å‚¬æ—¥ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã®race_idã‚’ç‰¹å®šï¼ˆÂ±1æ—¥ã‚·ãƒ•ãƒˆå¯¾å¿œï¼‰"""
    # 0. æ‰‹å‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ã‚’ãƒã‚§ãƒƒã‚¯
    if label and label in MANUAL_RACE_IDS:
        return MANUAL_RACE_IDS[label]

    # 1. å½“æ—¥ â†’ å‰æ—¥ â†’ ç¿Œæ—¥ ã®é †ã«æ¤œç´¢
    for day_offset in [0, -1, 1]:
        try:
            dt = datetime.strptime(date, "%Y%m%d") + timedelta(days=day_offset)
        except ValueError:
            continue
        shifted = dt.strftime("%Y%m%d")

        url = f"https://db.netkeiba.com/race/list/{shifted}/"
        content = safe_request(url)
        if content is None:
            continue

        soup = BeautifulSoup(content, "lxml")
        links = soup.select("a[href*='/race/']")

        for link in links:
            href = link.get("href", "")
            text = link.get_text(strip=True)
            match = re.search(r"/race/(\d{12})/", href)
            if match and keyword in text:
                if day_offset != 0:
                    print(f"  ğŸ“… æ—¥ä»˜ã‚·ãƒ•ãƒˆ: {date} â†’ {shifted} ã§ç™ºè¦‹")
                return match.group(1)

    return None


def _parse_race_info(soup: BeautifulSoup) -> tuple[str, str, str, str]:
    """ãƒ¬ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹åãƒ»è·é›¢ãƒ»surfaceãƒ»é¦¬å ´çŠ¶æ…‹ã‚’æŠ½å‡º"""
    # ãƒ¬ãƒ¼ã‚¹å
    race_name_tag = soup.select_one("h1")
    race_name = race_name_tag.get_text(strip=True) if race_name_tag else "ä¸æ˜"

    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆè¤‡æ•°ã‚»ãƒ¬ã‚¯ã‚¿å¯¾å¿œï¼‰
    race_data = soup.select_one(
        ".racedata, .data_intro, .RaceData01, .Race_Data01"
    )
    info_text = race_data.get_text() if race_data else ""

    distance = ""
    surface = ""
    going = ""
    dist_match = re.search(r"(èŠ|ãƒ€)(\d+)m", info_text)
    if dist_match:
        surface = "èŠ" if dist_match.group(1) == "èŠ" else "ãƒ€ãƒ¼ãƒˆ"
        distance = dist_match.group(2)
    going_match = re.search(r"(è‰¯|ç¨é‡|é‡|ä¸è‰¯)", info_text)
    if going_match:
        going = going_match.group(1)

    return race_name, distance, surface, going


def _scrape_from_race_netkeiba(race_id: str) -> pd.DataFrame | None:
    """race.netkeiba.com ã‹ã‚‰çµæœå–å¾—ï¼ˆä¸ŠãŒã‚Š3Fãƒ»é€šéé †ã‚ã‚Šï¼‰"""
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    content = safe_request(url)
    if content is None:
        return None

    soup = BeautifulSoup(content, "lxml")

    # race.netkeiba.comã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆè¤‡æ•°å€™è£œï¼‰
    table = soup.select_one(
        "table.RaceTable01, table.race_table_01, "
        "table.Shutuba_Table, table.ResultTableWrap"
    )
    if table is None:
        # tableã‚¿ã‚°ã‚’ã™ã¹ã¦èµ°æŸ»ã—ã¦ã€ãƒ˜ãƒƒãƒ€è¡Œã«ã€Œç€é †ã€ã‚’å«ã‚€ã‚‚ã®ã‚’æ¢ã™
        for t in soup.select("table"):
            header_text = t.get_text()[:200]
            if "ç€é †" in header_text and "é¦¬å" in header_text:
                table = t
                break

    if table is None:
        return None

    try:
        df = pd.read_html(StringIO(str(table)), header=0)[0]
    except Exception:
        return None

    # ä¸ŠãŒã‚Š3Fã¾ãŸã¯é€šéé †ã‚«ãƒ©ãƒ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    col_text = " ".join(str(c) for c in df.columns)
    has_last3f = any(k in col_text for k in ["ä¸Šã‚Š", "ä¸ŠãŒã‚Š", "ï¾—ï½½ï¾„"])
    has_passing = any(k in col_text for k in ["é€šé", "ï½ºï½°ï¾…ï½°"])

    if not has_last3f and not has_passing:
        return None  # DBç‰ˆã¨åŒç­‰ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸

    race_name, distance, surface, going = _parse_race_info(soup)

    df["race_id"] = race_id
    df["race_name"] = race_name
    df["distance"] = distance
    df["surface"] = surface
    df["going"] = going
    df["year"] = race_id[:4]

    return df


def _scrape_from_db_netkeiba(race_id: str) -> pd.DataFrame | None:
    """DBç‰ˆãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    url = f"https://db.netkeiba.com/race/{race_id}/"
    content = safe_request(url)
    if content is None:
        return None

    soup = BeautifulSoup(content, "lxml")
    race_name, distance, surface, going = _parse_race_info(soup)

    table = soup.select_one("table.race_table_01")
    if table is None:
        print(f"  âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«æœªæ¤œå‡º: {race_id}")
        return None

    try:
        df = pd.read_html(StringIO(str(table)), header=0)[0]
    except Exception as e:
        print(f"  âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«è§£æå¤±æ•—: {e}")
        return None

    df["race_id"] = race_id
    df["race_name"] = race_name
    df["distance"] = distance
    df["surface"] = surface
    df["going"] = going
    df["year"] = race_id[:4]

    return df


def scrape_race_result(race_id: str) -> pd.DataFrame | None:
    """ãƒ¬ãƒ¼ã‚¹çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ï¼ˆrace.netkeiba.comå„ªå…ˆã€DBç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    # ã¾ãš race.netkeiba.com ã‚’è©¦è¡Œï¼ˆä¸ŠãŒã‚Š3Fãƒ»é€šéé †ã‚ã‚Šï¼‰
    df = _scrape_from_race_netkeiba(race_id)
    if df is not None:
        return df

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: DBç‰ˆ
    return _scrape_from_db_netkeiba(race_id)


def clean_result_df(df: pd.DataFrame) -> pd.DataFrame:
    """ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–"""
    rename_map = {}
    for col in df.columns:
        s = str(col).strip().replace("\u3000", "")
        if "ç€é †" in s or s == "ç€é †":
            rename_map[col] = "ç€é †"
        elif s == "æ " or ("æ " in s and "ç•ª" in s):
            rename_map[col] = "æ ç•ª"
        elif s == "é¦¬ç•ª":
            rename_map[col] = "é¦¬ç•ª"
        elif "é¦¬å" in s:
            rename_map[col] = "é¦¬å"
        elif "æ€§é½¢" in s:
            rename_map[col] = "æ€§é½¢"
        elif "æ–¤é‡" in s:
            rename_map[col] = "æ–¤é‡"
        elif "é¨æ‰‹" in s:
            rename_map[col] = "é¨æ‰‹"
        elif s == "ã‚¿ã‚¤ãƒ ":
            rename_map[col] = "ã‚¿ã‚¤ãƒ "
        elif "ç€å·®" in s:
            rename_map[col] = "ç€å·®"
        elif "é€šé" in s:
            rename_map[col] = "é€šéé †"
        elif "ä¸Šã‚Š" in s or "ä¸ŠãŒã‚Š" in s or "å¾Œ3F" in s or s == "å¾Œ3F":
            rename_map[col] = "ä¸ŠãŒã‚Š3F"
        elif "å˜å‹" in s:
            rename_map[col] = "å˜å‹ã‚ªãƒƒã‚º"
        elif "äººæ°—" in s:
            rename_map[col] = "äººæ°—"
        elif "é¦¬ä½“é‡" in s:
            rename_map[col] = "é¦¬ä½“é‡"
        elif "èª¿æ•™å¸«" in s or "å©èˆ" in s:
            rename_map[col] = "èª¿æ•™å¸«"
        elif "è³é‡‘" in s:
            rename_map[col] = "è³é‡‘"
    if rename_map:
        df = df.rename(columns=rename_map)
    # æ®‹ã£ãŸã‚«ãƒ©ãƒ åã®ç©ºç™½é™¤å»
    df.columns = [str(c).replace(" ", "").replace("\u3000", "") for c in df.columns]
    return df


def main() -> None:
    print("=" * 50)
    print("SAKURA ORACLE - ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    print(f"å¯¾è±¡: {len(TARGET_RACES)}ãƒ¬ãƒ¼ã‚¹")
    print("=" * 50)

    all_results: list[pd.DataFrame] = []
    race_info_rows: list[dict] = []
    failed: list[str] = []

    pbar = tqdm(TARGET_RACES, desc="å–å¾—ä¸­")

    for race in pbar:
        label = race["label"]
        pbar.set_description(label)

        # Phase 1: race_id ç‰¹å®š
        race_id = find_race_id_from_date(race["date"], race["keyword"], label=label)
        if race_id is None:
            print(f"  âš ï¸ {label}: race_idæœªç‰¹å®š (æ—¥ä»˜: {race['date']})")
            failed.append(label)
            continue

        print(f"  âœ… {label} -> {race_id}")

        # Phase 2: çµæœå–å¾—
        df = scrape_race_result(race_id)
        if df is not None and not df.empty:
            df = clean_result_df(df)
            df["label"] = label
            all_results.append(df)
            race_info_rows.append({
                "race_id": race_id,
                "label": label,
                "date": race["date"],
                "distance": df["distance"].iloc[0] if "distance" in df.columns else "",
                "surface": df["surface"].iloc[0] if "surface" in df.columns else "",
                "going": df["going"].iloc[0] if "going" in df.columns else "",
                "num_horses": len(df),
            })
            print(f"    -> {len(df)}é ­å–å¾—")
        else:
            failed.append(label)

    pbar.close()

    # ä¿å­˜
    if all_results:
        combined = pd.concat(all_results, ignore_index=True)
        combined.to_csv(RAW_DIR / "race_results.csv", index=False, encoding="utf-8-sig")
        print(f"\nâœ… race_results.csv: {len(combined)}è¡Œ")

        info_df = pd.DataFrame(race_info_rows)
        info_df.to_csv(RAW_DIR / "race_info.csv", index=False, encoding="utf-8-sig")
        print(f"âœ… race_info.csv: {len(info_df)}ãƒ¬ãƒ¼ã‚¹")

    if failed:
        print(f"\nâš ï¸ å–å¾—å¤±æ•—: {len(failed)}ä»¶")
        for f in failed:
            print(f"   - {f}")

    print(f"\nåˆè¨ˆ: {len(all_results)}ãƒ¬ãƒ¼ã‚¹å–å¾—å®Œäº†")
    print("ãƒ‡ãƒ¼ã‚¿åé›†çµ‚äº†")


if __name__ == "__main__":
    main()
